{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "031d9c0c",
   "metadata": {},
   "source": [
    "# 1. Instaling dependecies via pip comand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9851eb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef03271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321a3e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6ad857",
   "metadata": {},
   "source": [
    "# 2. Loading environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f862a0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"CartPole-v0\"\n",
    "env =  gym.make (env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a06ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 5\n",
    "for episode in range (1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print(\"Episode: {} , Score: {}\".format(episode,score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108c2837",
   "metadata": {},
   "source": [
    "# 2.1 Understanding the Env\n",
    "\n",
    "https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb7c351",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space\n",
    "#this describes the space we have in the action within the env.\n",
    "#Discrete(2) means we have two possibles discrete values: 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cdabfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-push cart to left, 1-push cart to the right\n",
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d91757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space\n",
    "#this describes the space we have with in the observation with in the env.\n",
    "# Box means we have a list of lists, four values in each list, in a array of 4,0 and type flot32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e646e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [cart position, cart velocity, pole angle, pole angular velocity]\n",
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4e51f5",
   "metadata": {},
   "source": [
    "# 3. Training RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeabaa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38caa698",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1e7a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09b808f",
   "metadata": {},
   "source": [
    "# 4. Save and Reload Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca2bd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_Path = os.path.join('Training', 'Saved Models', 'PPO_model_cartpole')\n",
    "PPO_Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653e9f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PPO_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8681afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf916fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(PPO_Path, env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124dde4c",
   "metadata": {},
   "source": [
    "# 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591ff479",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1639de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8705e04c",
   "metadata": {},
   "source": [
    "# 6. Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a43ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 5\n",
    "for episode in range (1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print(\"Episode: {} , Score: {}\".format(episode,score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d95e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset() # reseting the initial conditions of environment\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c629f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "action, _ = model.predict(obs) # predicting an action with the new set of conditions/observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0d6669",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(action) # step with the action taken and reword of 1, we succeed in mainting the pole in the upright position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4e7818",
   "metadata": {},
   "source": [
    "# 7. Logs in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cbb490",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log_path = os.path.join(log_path, 'PPO_4') #last succesfull model trained.\n",
    "training_log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f191175",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!tensorboard --logdir = {training_log_path} # ! is a magic command. Issuing it here makes it execute in a command line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c4b42d",
   "metadata": {},
   "source": [
    "# 8. Call Back to stop on desired reward (training stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58230a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1083182",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('Training', 'Saved Models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbd32c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=200, verbose=1)\n",
    "eval_callback = EvalCallback(env,\n",
    "                            callback_on_new_best=stop_callback,\n",
    "                            eval_freq=10000,\n",
    "                            best_model_save_path=save_path,\n",
    "                            verbose=1\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33b9d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45349b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979848a7",
   "metadata": {},
   "source": [
    "   ## 8.1 Testing model with callback on rewardthreshold=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60332fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 5\n",
    "for episode in range (1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print(\"Episode: {} , Score: {}\".format(episode,score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d606baf6",
   "metadata": {},
   "source": [
    "# 9 Changing Policies (network architecture of MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f01aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_arch=[dict(pi=[128, 128, 128, 128], vf=[128, 128, 128, 128])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6df39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log=log_path, policy_kwargs={'net_arch': net_arch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44dab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c3ff55",
   "metadata": {},
   "source": [
    "## 9.1 Testing with the new MLP architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c264781",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 5\n",
    "for episode in range (1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print(\"Episode: {} , Score: {}\".format(episode,score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c267a7",
   "metadata": {},
   "source": [
    "# 10. Alternate Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1473a10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf948f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DQN = DQN('MlpPolicy', env, verbose = 1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d49df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DQN.learn(total_timesteps=20000, log_interval=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75747cd",
   "metadata": {},
   "source": [
    "## 10.1 Testing DQN Algorithm trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dbeb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 5\n",
    "for episode in range (1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _ = model_DQN.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print(\"Episode: {} , Score: {}\".format(episode,score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39054c8",
   "metadata": {},
   "source": [
    "### as we can see this model do not performs very well with the above settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f8892f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN_Path = os.path.join('Training', 'Saved Models', 'DQN_model_cartpole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f1a6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DQN.save(DQN_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1543ad7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
